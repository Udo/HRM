torch
# Optional (CUDA only): flash-attn / flash_attention_interface for faster attention.
# On macOS / CPU these are skipped automatically.
adam-atan2
einops
tqdm
coolname
pydantic
argdantic
wandb
omegaconf
hydra-core
huggingface_hub
Pillow
